{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import numpy.random\n",
    "import random\n",
    "import argparse\n",
    "import chainer\n",
    "import json\n",
    "import datetime\n",
    "from chainer import optimizers\n",
    "import chainer.functions as F\n",
    "import chainer.links as L\n",
    "from chainer import serializers\n",
    "from chainer.functions import caffe\n",
    "from chainer import cuda\n",
    "from chainer import Variable\n",
    "from chainer import computational_graph\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "パスを扱いやすくするために home_dir(フォルダの根幹) を定義<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "home_dir = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# HACK : ipynbでargs.を動かすためにクラスを用いた。 pyファイルではコメントアウトする　　\n",
    "#ー補足ー easydictで同じ処理を行える。\n",
    "class args:\n",
    "    def __init__(self, finetune, crawl, gpu, batchsize, test_val_ratio, n_epoch,\n",
    "                 augment, finetune_model_lr, my_model_lr, finetune_net,\n",
    "                 thrown_away_layer, optimizer1, optimizer2):\n",
    "        self.finetune = finetune\n",
    "        self.crawl = crawl\n",
    "        self.gpu = gpu\n",
    "        self.batchsize = batchsize\n",
    "        self.test_val_ratio = test_val_ratio\n",
    "        self.n_epoch = n_epoch\n",
    "        self.augment = augment\n",
    "        self.finetune_model_lr = finetune_model_lr\n",
    "        self.my_model_lr = my_model_lr\n",
    "        self.finetune_net = finetune_net\n",
    "        self.output_layer = output_layer\n",
    "        self.optimizer1 = optimizer1\n",
    "        self.optimizer2 = optimizer2\n",
    "        self.rotate = rotate\n",
    "        self.flip_x = flip_x\n",
    "        self.flip_y = flip_y\n",
    "        self.translation = translation\n",
    "        self.gaussian_noise = gaussian_noise\n",
    "        self.variance = variance\n",
    "        self.zoom = zoom\n",
    "        self.fix_sample = fix_sample\n",
    "        self.val_aug = val_aug\n",
    "        self.tr_aug = tr_aug\n",
    "        self.inv_aug = inv_aug\n",
    "\n",
    "args.finetune = True\n",
    "args.crawl = False\n",
    "args.gpu = -1\n",
    "args.batchsize = 100\n",
    "args.test_val_ratio = 0.15\n",
    "args.n_epoch = 10\n",
    "args.augment = False\n",
    "args.finetune_model_lr = 1e-4\n",
    "args.my_model_lr = 1e-3\n",
    "args.finetune_net = 'squeezenet'\n",
    "args.output_layer = 'pool8'\n",
    "args.optimizer1 = 'MomentumSGD'\n",
    "args.optimizer2 = 'MomentumSGD'\n",
    "args.rotate = True\n",
    "args.flip_x = True\n",
    "args.flip_y = True\n",
    "args.translation = True\n",
    "args.gaussian_noise = True\n",
    "args.variance = True\n",
    "args.zoom = True\n",
    "args.fix_sample = 1\n",
    "args.val_aug = True\n",
    "args.tr_aug = True\n",
    "args.inv_aug = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def arg_process(argument):\n",
    "    parser = argparse.ArgumentParser(description='VDT Project')\n",
    "    parser.add_argument('--finetune',type=bool, default=True,\n",
    "                    help = 'finetune or not')\n",
    "    parser.add_argument('--crawl', '-c', type=bool, default=False,\n",
    "                    help ='adding crawling data or not')\n",
    "    parser.add_argument('--gpu', '-g', default=0, type=int,\n",
    "                    help='GPU ID (negative value indicates CPU)')\n",
    "    parser.add_argument('--batchsize', '-b', default=20, type=int, \n",
    "                    help = 'batchsize')\n",
    "    parser.add_argument('--test_val_ratio', '-t', default = 0.15, type = float, \n",
    "                    help = 'val/train_ratio')\n",
    "    parser.add_argument('--n_epoch', '-n', default = 100, type = int, \n",
    "                    help = 'number of epoch')\n",
    "    parser.add_argument('--augment', '-a', default =False, type = bool, \n",
    "                    help = 'augment or not')\n",
    "    parser.add_argument('--finetune_model_lr', '-f', default =1e-4, type = float, \n",
    "                    help = 'learning rate of finetune model')\n",
    "    parser.add_argument('--my_model_lr', '-m', default = 1e-3, type = float, \n",
    "                    help = 'learning rate of model i added')\n",
    "    parser.add_argument('--finetune_net', '-fn', default = 'Alexnet', type = str,\n",
    "                    help = 'net i finetune')\n",
    "    parser.add_argument('--output_layer', '-out', default = 'fc7', type = str,\n",
    "                    help = 'output layer')\n",
    "    parser.add_argument('--optimizer1', '-o1', default = 'MomentumSGD', type =str,\n",
    "                    help = 'optimizer used for finetune model')\n",
    "    parser.add_argument('--optimizer2', '-o2', default = 'MomentumSGD', type = str,\n",
    "                    help = 'optimizer used for my model')\n",
    "    parser.add_argument('--rotate','-r',default=False,type=bool,\n",
    "                    help='rotate augmentation')\n",
    "    parser.add_argument('--flip_x','-fx',default=False,type=bool,\n",
    "                    help='flip_x augmentation')\n",
    "    parser.add_argument('--flip_y','-fy',default=False,type=bool,\n",
    "                    help='flip_y augmentation')\n",
    "    parser.add_argument('--translation','-trans',default=False,type=bool,\n",
    "                    help='translation augmentation')\n",
    "    parser.add_argument('--gaussian_noise','-gau',default=False,type=bool,\n",
    "                    help='gaussian noise augmentation')\n",
    "    parser.add_argument('--variance','-v',default=False,type=bool,\n",
    "                    help='variance augmentation')\n",
    "    parser.add_argument('--zoom','-z',default=False,type=bool,\n",
    "                    help='zoom augmentation')\n",
    "    parser.add_argument('--fix_sample','-fix',default= 0 ,type=int,\n",
    "                    help='fix sample')\n",
    "    parser.add_argument('--val_aug','-val',default=True,type=bool,\n",
    "                    help='val augmentation')\n",
    "    parser.add_argument('--tr_aug','-tr',default=True,type=bool,\n",
    "                    help='train augmentation')\n",
    "    parser.add_argument('--inv_aug','-inv',default=True,type=bool,\n",
    "                    help='inv augmentation')\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    return args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデルグラフ生成用のデータ入力"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_computational_graph(loss):\n",
    "    with open(home_dir + '/output/graph.dot', 'w') as o:\n",
    "        o.write(chainer.computational_graph.build_computational_graph((loss, )).dump())\n",
    "    with open(home_dir + '/output/graph.wo_split.dot', 'w') as o:\n",
    "        g = chainer.computational_graph.build_computational_graph((loss, ), remove_split=True)\n",
    "        o.write(g.dump())\n",
    "    print('graph generated')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## accuracy・loss のログを書き込む関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_log(type):\n",
    "    if type == 'train':    \n",
    "        ss = json.dumps({'type' : type, 'samples': n_imgs_trained,\n",
    "                                       'accuracy': sum_accuracy/sum_n, 'loss': sum_loss/sum_n}) + '\\n'\n",
    "    if type == 'inv':\n",
    "        ss = json.dumps({'type': type, 'samples': n_imgs_trained,\n",
    "                                        'accuracy': inv_acc, 'loss': inv_loss}) + '\\n'\n",
    "    if type == 'val':\n",
    "        ss = json.dumps({'type': type, 'samples': n_imgs_trained,\n",
    "                                        'accuracy': val_acc, 'loss': val_loss}) + '\\n'\n",
    "    sum_accuracy = 0\n",
    "    sum_loss = 0\n",
    "    sum_n = 0\n",
    "    logfile.write(ss)\n",
    "    logfile.flush()\n",
    "    print(ss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 計算条件の保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def log_calc(args):\n",
    "    today = str(datetime.datetime.today())\n",
    "    today = today.replace(' ', ',')\n",
    "    today = today.replace(',', '_')\n",
    "    today = today.replace('.', '_')\n",
    "    today = today.replace(':', '_')\n",
    "    today = today.replace('/', '_')\n",
    "    logfile = open(home_dir + '/output/{}.log'.format(today), 'a')\n",
    "    calc_cond = json.dumps({'crawl': args.crawl, \n",
    "                                                 'gpu': args.gpu,\n",
    "                                                 'batchsize' : args.batchsize,\n",
    "                                                 'test_val_ratio': args.test_val_ratio,\n",
    "                                                 'n_epoch': args.n_epoch,\n",
    "                                                 'augment': args.augment,\n",
    "                                                 'finetune_model_lr': args.finetune_model_lr,\n",
    "                                                 'my_model_lr': args.my_model_lr, \n",
    "                                                 'finetune_net': args.finetune_net, \n",
    "                                                 'output_layer': args.output_layer, \n",
    "                                                 'optimizer1': args.optimizer1,\n",
    "                                                 'optimizer2': args.optimizer2,\n",
    "                                                 'rotate':args.rotate,\n",
    "                                                 'flip_x':args.flip_x,\n",
    "                                                 'flip_y':args.flip_y,\n",
    "                                                 'translation':args.translation,\n",
    "                                                 'gaussian_noise':args.gaussian_noise,\n",
    "                                                 'variance':args.variance,\n",
    "                                                 'zoom':args.zoom,\n",
    "                                                 'fix_sample':args.fix_sample,\n",
    "                                                 'val_aug':args.val_aug,\n",
    "                                                 'tr_aug':args.tr_aug, \n",
    "                                                 'inv_aug':args.inv_aug}) + '\\n'\n",
    "    logfile.write(calc_cond)\n",
    "    logfile.flush()\n",
    "    print(calc_cond)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データの読み込み\n",
    "数の多い上位12クラスのClassラベル・同被写体ラベル(object_label)・切り取りの行われている画像(Top12_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data(args):\n",
    "    f = open(os.path.join(home_dir + '/data/Top12_label.pkl'), 'rb')\n",
    "    Top12_label = pickle.load(f)\n",
    "    f.close()       \n",
    "    f = open(os.path.join(home_dir + '/data/Top12_object_label.pkl'), 'rb')\n",
    "    Top12_object_label = pickle.load(f)\n",
    "    f.close()\n",
    "    Top12_object_label = list(Top12_object_label)\n",
    "    Top12_label = list(Top12_label)\n",
    "\n",
    "    return Top12_object_label, Top12_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  切り取り処理後の画像の読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Top12_processed_data(args):\n",
    "    Top12_processed = np.load(os.path.join(home_dir + '/data/Top12_processed.npy'))\n",
    "    return Top12_processed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データを評価用と学習用に分ける関数\n",
    "input: test用データ比率、　output: test用,training用の(fail名,Class名)と object_label のリスト"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def make_train_val(args,split_rate=args.test_val_ratio):\n",
    "    train_object_label = []\n",
    "    val_object_label = []\n",
    "    file_name_train = []\n",
    "    file_name_test = []\n",
    "    Top12_object_label, Top12_label, Top12_processed = load_data(args)\n",
    "    # file名は０から順に数字が振られている。\n",
    "    file_name = np.arange(len(Top12_label))\n",
    "    \n",
    "    for i in range(12):\n",
    "        # Top12_object_labelの要素の中でClassがiであるものを重複なくi_labelに取り出す。\n",
    "        i_label = np.unique(np.array(Top12_object_label)[np.array(Top12_label) == i ])\n",
    "        # i_labelの中からtest用データ分のobject_labelをi_label_testに入れる。\n",
    "        i_label_test = np.random.choice(i_label, int(np.round(i_label.shape[0]*split_rate)), replace = False)\n",
    "        # i_labelの中でi_label_testに入っていないものをi_label_trainに入れる。\n",
    "        for j in range(len(i_label_test)):\n",
    "            i_label = i_label[i_label != i_label_test[j]]\n",
    "        i_label_train = i_label \n",
    "        train_list = np.in1d(Top12_object_label,i_label_train)\n",
    "        test_list = np.in1d(Top12_object_label,i_label_test)\n",
    "        file_name_train += [(name, class_i) for name,class_i in zip(file_name[train_list], Top12_label[train_list])]\n",
    "        file_name_test += [(name, class_i) for name,class_i in zip(file_name[test_list], Top12_label[test_list])]\n",
    "        val_object_label += [object_label for object_label in Top12_object_label[train_list]]\n",
    "        train_object_label += [object_label for object_label in Top12_object_label[test_list]]\n",
    "    return file_name_train, file_name_test, val_object_label, train_object_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 評価用・学習用データの決定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_train_val(args):\n",
    "    # 固定された評価用・学習用データを用いる。\n",
    "    if args.fix_sample:\n",
    "        f = open(os.path.join(home_dir+'/data/train_list_'+str(args.fix_sample)+'.pkl'),'rb')\n",
    "        train_list = pickle.load(f)\n",
    "        f.close()\n",
    "        f = open(os.path.join(home_dir+'/data/val_list_'+str(args.fix_sample)+'.pkl'),'rb')\n",
    "        val_list = pickle.load(f)\n",
    "        f.close()\n",
    "        f = open(os.path.join(home_dir+'/data/train_object_label_'+str(args.fix_sample)+'.pkl'),'rb')\n",
    "        train_object_label = pickle.load(f)\n",
    "        f.close()\n",
    "        f = open(os.path.join(home_dir+'/data/val_object_label_'+str(args.fix_sample)+'.pkl'),'rb')\n",
    "        val_object_label = pickle.load(f)\n",
    "        f.close()\n",
    "\n",
    "    # random な分け方で得られた評価用・学習用データを用いる。   \n",
    "    else:\n",
    "        train_list, val_list, val_object_label, train_object_label = make_train_val(args)\n",
    "    #評価用データを10 倍にしてvalidationを行う。\n",
    "    if args.val_aug:\n",
    "        val_list_10 = []\n",
    "        for i in val_list:\n",
    "            val_list_10 += [i] * 10\n",
    "        val_list = val_list_10\n",
    "    return train_list, val_list, val_object_label, train_object_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 画像の枚数を調整する\n",
    "学習時に各個体の画像数に大きな差があると個体に対しての過学習が大きくなり、全ての個体を同じ枚数だけ学習させようとすると、画像に対しての過学習が大きくなるため、各個体の画像数が最大枚数(max_count)＊(その個体の画像数)の平方根となるようにした。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def random_aug(train_list, train_object_label):\n",
    "    transposed_train = np.array(train_list).transpose()\n",
    "    aug_train_list = []\n",
    "    count = 0\n",
    "    values,count_num = np.unique(train_object_label,return_counts=True)\n",
    "    max_count = 171\n",
    "    for i in range(len(count_num)):\n",
    "        n = count_num[i]\n",
    "        stay_n,random_n = divmod(np.sqrt(n*max_count),n)\n",
    "        random_list = np.arange(count,count+n)\n",
    "        random_box = np.random.choice(random_list,int(random_n),replace=False)\n",
    "        aug_train_list.append(train_list[count:count+n]*int(stay_n))\n",
    "        for j in random_box:\n",
    "            aug_train_list.append([train_list[j]])  \n",
    "        count += n\n",
    "    train_list = [e for inner_list in aug_train_list for e in inner_list]\n",
    "    return train_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  epoch ごとにrandom_aug関数で作ったtrain_listを格納する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_all_train(train_list ,train_object_label):\n",
    "    all_aug_train = []\n",
    "    for i in range(args.n_epoch):\n",
    "        aug_train= random_aug(train_list, train_object_label)\n",
    "        random.shuffle(aug_train)\n",
    "        all_aug_train += [aug_train]\n",
    "    random.shuffle(all_aug_train)\n",
    "    return all_aug_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## augmentationを行う関数\n",
    "input : 画像の配列、　output : augmentを行った後の画像の配列<br>\n",
    "augmentの種類<br>\n",
    "回転・x軸反転・y軸反転・並進移動・ガウシアンノイズ・分散調節・拡大"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_image(image,split_ratio=None):\n",
    "    crop_size = 0.8\n",
    "    insize = 227\n",
    "    x_size= int(insize*(1-crop_size))\n",
    "    y_size = int(insize*(1-crop_size))\n",
    "    width = int(insize-2*x_size)\n",
    "    height =int(insize-2*y_size)\n",
    "    if args.augment:\n",
    "        is_rotate = np.random.randint(0, args.rotate + 1)\n",
    "        is_flip_x = np.random.randint(0, args.flip_x +1 )\n",
    "        is_flip_y = np.random.randint(0, args.flip_y +1)\n",
    "        is_trans =  np.random.randint(0, args.translation +1)\n",
    "        is_gaussian= np.random.randint(0, args.gaussian_noise +1)\n",
    "        is_variance= np.random.randint(0,args.variance+1)\n",
    "        is_zoom = np.random.randint(0,args.zoom+1)\n",
    "\n",
    "        if is_rotate: #  Transpose X and Y axis\n",
    "            image =  image.transpose(0, 2, 1)\n",
    "        if is_flip_x: # Flip along Y-axis\n",
    "            image = np.array(cv2.flip(image, 1))\n",
    "        if is_flip_y: # Flip along X-axis\n",
    "            image = image[:, :, ::-1]\n",
    "        if is_trans:#translation move\n",
    "            M = np.float32([[1,0,random.randint(-30, 30)],[0,1,random.randint(-30, 30)]])\n",
    "            image = image.transpose(1,2,0)\n",
    "            image = np.array(cv2.warpAffine(image, M, (insize, insize))).transpose(2,0,1)\n",
    "        if is_gaussian:\n",
    "            gauss = np.random.normal(0,0.05,(3,insize,insize))\n",
    "            image +=  gauss\n",
    "        if is_variance:\n",
    "            image /= 0.5\n",
    "        if is_zoom:\n",
    "            src = image.transpose(1,2,0)\n",
    "            dst = src[y_size:y_size+height, x_size:x_size+width]\n",
    "            image = cv2.resize(dst,(insize,insize)).transpose(2,0,1)\n",
    "        return image\n",
    "    else:\n",
    "        return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 説明変数と目的変数に分ける関数\n",
    "input : (file名,label)のタプルが格納されたリスト、output : 説明変数(x_batch),目的変数(y_batch)<br>\n",
    "x_batch.shape = (sample数,channel,hight,width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_xy(ff, augment=args.augment):\n",
    "    x_batch = []; y_batch = []\n",
    "    for fl in ff:\n",
    "        # fl= (filename, label)\n",
    "        x_batch += [read_image(Top12_processed[fl[0]],augment)]               \n",
    "        y_batch += [fl[1]]              \n",
    "    return np.array(x_batch), np.array(y_batch, dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this is not necessary except for in the tyt instances\n",
    "os.environ[\"PATH\"] = '/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 重み更新を行う層の決定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_layer(args):\n",
    "    layer_list = func.__dict__['_children']\n",
    "    LAYER = args.output_layer\n",
    "    if args.output_layer == 'fc7':\n",
    "        disable = ['fc8']\n",
    "    elif args.output_layer == 'fc6':\n",
    "        disable = ['fc7', 'fc8']\n",
    "    elif args.output_layer == 'pool5':\n",
    "        disable = ['fc6', 'fc7', 'fc8']\n",
    "    elif args.output_layer == 'conv10':\n",
    "        disable = ['pool10']\n",
    "    elif args.output_layer == 'fire9/expand3x3':\n",
    "        disable = ['conv10', 'pool10']\n",
    "    elif args.output_layer == 'pool8':\n",
    "        disable = ['fire9', 'conv10', 'pool10']\n",
    "    elif args.output_layer == 'res3d_branch2c':\n",
    "        disable = layer_list[70:160]\n",
    "    elif args.output_layer == 'res4f_branch2c':\n",
    "        disable = layer_list[127:160]\n",
    "    elif args.output_layer == 'res5c_branch2c':\n",
    "        disable = layer_list[157:160]\n",
    "    return disable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 順伝播関数\n",
    "input : 画像配列,目的変数、　output : pred=True - 推測結果,  train=True - lossとaccuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forward(im_data, y_data, train=True, pred=False):\n",
    "    disable = get_layer(args)\n",
    "    LAYER = args.output_layer\n",
    "    im_data = xp.array(im_data, dtype=np.float32)\n",
    "    im = chainer.Variable(im_data, volatile=not args.finetune)\n",
    "    x, = func(inputs={'data': im}, outputs=[LAYER], disable=disable, train=train)\n",
    "    if args.finetune and train: \n",
    "        y = model.r1(x)\n",
    "    else:\n",
    "        y = model.r1(chainer.Variable(x.data, volatile=not train))\n",
    "    t = chainer.Variable(xp.array(y_data), volatile=not train)    # assume that y_data is np.int32\n",
    "    loss = F.softmax_cross_entropy(y, t)\n",
    "    acc = F.accuracy(y, t)\n",
    "    if pred: \n",
    "        return y\n",
    "    else:\n",
    "        return loss, acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 精度評価関数(画像)\n",
    "各画像に対して精度を評価するための関数<br>\n",
    "同被写体について枚数の偏りを考慮して各個体に対して√ni/Σ√niだけの重みを付けて精度を算出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_loss_acc(pred, y_batch, object_val_array):\n",
    "    object_val_list = list(object_val_array)\n",
    "    uniq_val_list = list(np.unique(object_val_array))\n",
    "    N = len(uniq_val_list)\n",
    "    loss = 0\n",
    "    acc = 0\n",
    "    weight_list = []\n",
    "    sum = 0\n",
    "    for i in range(N):\n",
    "        num = object_val_list.count(uniq_val_list[i])\n",
    "        weight_list.append(np.sqrt(num))\n",
    "        sum += np.sqrt(num)\n",
    "    for i in range(N):\n",
    "        y = Variable(xp.array(pred[object_val_array == uniq_val_list[i]].astype(np.float32)))\n",
    "        t = Variable(xp.array(y_batch[object_val_array == uniq_val_list[i]].astype(np.int32)))\n",
    "        loss += F.softmax_cross_entropy(y, t)*weight_list[i]/sum\n",
    "        acc += F.accuracy(y, t)*weight_list[i]/sum\n",
    "    return float(chainer.cuda.to_cpu(loss.data)) , float(chainer.cuda.to_cpu(acc.data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 精度評価関数(個体)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def inv_loss_acc(pred_all, y_batch_all, object_val_array):\n",
    "    val_unique, num = np.unique(object_val_array, return_counts=True)\n",
    "    for value in (val_unique):\n",
    "        bool =  object_val_array == value\n",
    "        if value == val_unique[0]:\n",
    "            inv_pred = np.array([np.mean(pred_all[bool], axis=0)])\n",
    "            new_y_batch = np.array(np.mean(y_batch_all[bool]))\n",
    "        else:\n",
    "            inv_pred = np.concatenate((inv_pred, np.array([np.mean(pred_all[bool], axis =0)])))\n",
    "            new_y_batch = np.append(new_y_batch,np.array(np.mean(y_batch_all[bool])))\n",
    "    y = Variable(xp.array(inv_pred.astype(np.float32)))\n",
    "    t = Variable(xp.array(new_y_batch.astype(np.int32)))\n",
    "    loss = F.softmax_cross_entropy(y, t)\n",
    "    acc = F.accuracy(y, t)\n",
    "    return float(chainer.cuda.to_cpu(loss.data)) , float(chainer.cuda.to_cpu(acc.data))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 十倍のバリデーションを行なった際の予測"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_pred_av(pred_all):\n",
    "    for i in range(0, len(pred_all), 10):\n",
    "        if i == 0:\n",
    "            pred_av = np.array([np.mean(pred_all[i : i+10], axis = 0)])\n",
    "        else:\n",
    "            pred_av = np.concatenate((pred_av,np.array([np.mean(pred_all[i : i+10], axis = 0)])))\n",
    "    return pred_av"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデルの読み込み・生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_model(args):\n",
    "    if args.finetune_net == 'Alexnet':\n",
    "        #func = caffe.CaffeFunction(home_dir+\"/Model_data/bvlc_reference_caffenet.caffemodel\")\n",
    "        #f = open(home_dir + \"/Model_data/caffemodel.pickle\", 'wb')\n",
    "        #pickle.dump(func, f)\n",
    "        #f.close()\n",
    "        f = open(home_dir + \"/Model_data/caffemodel.pickle\", 'rb')\n",
    "        func = pickle.load(f)\n",
    "        f.close()\n",
    "    elif args.finetune_net == 'squeezenet':\n",
    "        #func = caffe.CaffeFunction(home_dir+\"/Model_data/squeezenet_v1.0.caffemodel\")\n",
    "        #f= open(home_dir + \"/Model_data/squeezenet_1.0.pickle\", 'wb')\n",
    "        #pickle.dump(func, f)\n",
    "        #f.close()\n",
    "        f = open(home_dir + \"/Model_data/squeezenet_1.0.pickle\", 'rb')\n",
    "        func = pickle.load(f)\n",
    "        f.close()\n",
    "    elif args.finetune_net == 'Resnet':\n",
    "        #func = caffe.CaffeFunction(home_dir+\"/Model_data/ResNet-50-model.caffemodel\")\n",
    "        #f= open(home_dir + \"/Model_data/Resnet-50-model.pickle\", 'wb')\n",
    "        #pickle.dump(func, f)\n",
    "        #f.close()\n",
    "        f = open(home_dir + \"/Model_data/Resnet-50-model.pickle\", 'rb')\n",
    "        func = pickle.load(f)\n",
    "        f.close()\n",
    "    if args.output_layer == 'fc7':\n",
    "        output_num = 4096\n",
    "    elif args.output_layer == 'fc6':\n",
    "        output_num = 4096\n",
    "    elif args.output_layer == 'pool5':\n",
    "        output_num =9216\n",
    "    elif args.output_layer == 'conv10':\n",
    "        output_num = 225000\n",
    "    elif args.output_layer == 'fire9/expand3x3':\n",
    "        output_num = 43264\n",
    "    elif args.output_layer == 'pool8':\n",
    "        output_num =86528\n",
    "    elif args.output_layer == 'res3d_branch2c':\n",
    "        output_num = 430592\n",
    "    elif args.output_layer == 'res4f_branch2c':\n",
    "        output_num = 230400\n",
    "    elif args.output_layer == 'res5c_branch2c':\n",
    "        output_num = 131072\n",
    "    n_ch = 12\n",
    "    model = chainer.FunctionSet( r1 = L.Linear(output_num, n_ch))\n",
    "    if args.gpu >= 0:\n",
    "        model.to_gpu()\n",
    "        func.to_gpu()\n",
    "\n",
    "    return func, model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## optimizerの決定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_optimizer(func, model):\n",
    "    #optimizer = optimizers.Adam()\n",
    "    #optimizer = optimizers.RMSprop()\n",
    "    optimizer = optimizers.MomentumSGD(lr = args.finetune_model_lr)\n",
    "    optimizer2 = optimizers.MomentumSGD(lr = args.my_model_lr)\n",
    "    optimizer.setup(model)\n",
    "    optimizer2.setup(func)\n",
    "    return optimizer, optimizer2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習・予測"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run(all_aug_train,N,Top12_processed):\n",
    "    global n_imgs_trained\n",
    "    interval_log = 1000\n",
    "    n_val = 4\n",
    "    n_imgs_trained = 0 \n",
    "    n_epoch = args.n_epoch\n",
    "    sum_accuracy = 0\n",
    "    sum_loss = 0\n",
    "    sum_n = 0\n",
    "    n_tr = 0 \n",
    "    batchsize = args.batchsize\n",
    "    t_batchsize = args.batchsize\n",
    "    if args.finetune_net =='Resnet':\n",
    "        t_batchsize = 1\n",
    "\n",
    "\n",
    "    N_test = len(val_list)\n",
    "    \n",
    "    for epoch in tqdm(range(n_epoch)):\n",
    "        print('epoch', epoch)\n",
    "        #training\n",
    "        perm = np.random.permutation(N)\n",
    "        for i in tqdm(range(0, N, batchsize)):  \n",
    "            if i % 200==0:\n",
    "                print('batch', i)\n",
    "            x_batch, y_batch = read_xy(all_aug_train[epoch][perm[i : i+batchsize]], augment=True)\n",
    "            optimizer.zero_grads()\n",
    "            optimizer2.zero_grads()\n",
    "            loss, acc = forward(x_batch, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.update()\n",
    "            optimizer2.update()\n",
    "        \n",
    "            if epoch ==1 and i==0: \n",
    "                write_computational_graph(loss)\n",
    "            sum_loss  += float(chainer.cuda.to_cpu(loss.data)) * len(y_batch)\n",
    "            sum_accuracy += float(chainer.cuda.to_cpu(acc.data)) * len(y_batch)\n",
    "            sum_n += len(y_batch)\n",
    "            n_imgs_trained += len(y_batch)\n",
    "            if sum_n >=interval_log:\n",
    "                write_log('train')\n",
    "                n_tr += 1\n",
    "                if n_tr % n_val == 0:\n",
    "                    serializers.save_npz(home_dir + '/output/' + today + '_model', model)\n",
    "                    serializers.save_npz(home_dir + '/output/' + today + '_optimizer', optimizer)\n",
    "                    serializers.save_npz(home_dir + '/output/' + today + '_func', func)\n",
    "                    serializers.save_npz(home_dir + '/output/' + today + '_optimizer2', optimizer2)\n",
    "                    # valuation\n",
    "                    pred_all = None\n",
    "                    y_batch_all = None\n",
    "                    for ii in tqdm(range(0, N_test, t_batchsize)):\n",
    "                        x_batch, y_batch = read_xy(val_list[ii : ii+t_batchsize], augment=True) \n",
    "                        pred_array = forward(x_batch, y_batch, train = False, pred = True).data\n",
    "                    \n",
    "                        if pred_all is None:\n",
    "                            pred_all = chainer.cuda.to_cpu(pred_array)\n",
    "                        else:\n",
    "                            pred_all = np.concatenate((pred_all, chainer.cuda.to_cpu(pred_array)))\n",
    "                        if y_batch_all is None:\n",
    "                            y_batch_all = y_batch\n",
    "                        else:\n",
    "                            y_batch_all = np.concatenate((y_batch_all, y_batch))\n",
    "                    pred_av = get_pred_av(pred_all)\n",
    "                    inv_loss, inv_acc = inv_loss_acc(pred_av, y_batch_all[0:len(y_batch_all):10], np.array(val_object_label))\n",
    "                    val_loss, val_acc = get_loss_acc(pred_av, y_batch_all[0:len(y_batch_all):10], np.array(val_object_label))\n",
    "                    sum_n += len(y_batch)\n",
    "                \n",
    "                    write_log('val')\n",
    "                    write_log('inv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    global Top12_processed,func,model,optimizer,optimizer2,xp,train_list,val_list\n",
    "    #args = arg_process(sys.argv[1:])\n",
    "    train_list, val_list, val_object_label, train_object_label = get_train_val(args)\n",
    "    Top12_processed = Top12_processed_data(args)\n",
    "    all_aug_train = get_all_train(train_list,train_object_label)\n",
    "    func, model = get_model(args)\n",
    "    optimizer, optimizer2 = get_optimizer(func, model)\n",
    "    if args.gpu >= 0:\n",
    "        cuda.get_device(args.gpu).use()\n",
    "        xp = cuda.cupy\n",
    "    else:\n",
    "        xp = np\n",
    "\n",
    "    val_list = np.array(val_list)\n",
    "    N = len(all_aug_train[0])\n",
    "    all_aug_train = np.array(all_aug_train)\n",
    "    \n",
    "    run(all_aug_train,N,Top12_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "  0%|          | 0/55 [00:00<?, ?it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0\n",
      "batch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  2%|▏         | 1/55 [01:54<1:43:19, 114.81s/it]\u001b[A\n",
      "  4%|▎         | 2/55 [04:28<1:51:42, 126.47s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  5%|▌         | 3/55 [06:38<1:50:32, 127.55s/it]\u001b[A\n",
      "  7%|▋         | 4/55 [09:07<1:53:55, 134.04s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  9%|▉         | 5/55 [10:51<1:44:13, 125.06s/it]\u001b[A\n",
      " 11%|█         | 6/55 [12:51<1:40:43, 123.34s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 13%|█▎        | 7/55 [14:44<1:36:17, 120.37s/it]\u001b[A\n",
      " 15%|█▍        | 8/55 [17:00<1:37:55, 125.00s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 16%|█▋        | 9/55 [19:07<1:36:17, 125.59s/it]\u001b[A\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'sum_accuracy' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-58ca95c5b364>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-28-f4273e19c201>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mall_aug_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_aug_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_aug_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mTop12_processed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-27-5773fd15fc01>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(all_aug_train, N, Top12_processed)\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0mn_imgs_trained\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0msum_n\u001b[0m \u001b[0;34m>=\u001b[0m\u001b[0minterval_log\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m                 \u001b[0mwrite_log\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m                 \u001b[0mn_tr\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mn_tr\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mn_val\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-76d3a2ce99ee>\u001b[0m in \u001b[0;36mwrite_log\u001b[0;34m(type)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         ss = json.dumps({'type' : type, 'samples': n_imgs_trained,\n\u001b[0;32m----> 4\u001b[0;31m                                        'accuracy': sum_accuracy/sum_n, 'loss': sum_loss/sum_n}) + '\\n'\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'inv'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         ss = json.dumps({'type': type, 'samples': n_imgs_trained,\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'sum_accuracy' referenced before assignment"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
